{
  
    
        "post0": {
            "title": "CLASSIFICATION OF TRAFFIC SIGNS USING LE-NET ARCHITECTURE IN KERAS.",
            "content": "In this project we use the traffic signs to train a Deep Network to classify them. . The dataset contains 43 different classes of images. . Loading Datasets and Libraries. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pickle . with open(&quot;./traffic-signs-data/train.p&quot;, mode=&#39;rb&#39;) as training_data: train = pickle.load(training_data) with open(&quot;./traffic-signs-data/valid.p&quot;, mode=&#39;rb&#39;) as validation_data: valid = pickle.load(validation_data) with open(&quot;./traffic-signs-data/test.p&quot;, mode=&#39;rb&#39;) as testing_data: test = pickle.load(testing_data) . X_train, y_train = train[&#39;features&#39;], train[&#39;labels&#39;] X_validation, y_validation = valid[&#39;features&#39;], valid[&#39;labels&#39;] X_test, y_test = test[&#39;features&#39;], test[&#39;labels&#39;] . X_train.shape . (34799, 32, 32, 3) . y_train.shape . (34799,) . X_validation.shape . (4410, 32, 32, 3) . y_validation.shape . (4410,) . X_test.shape . (12630, 32, 32, 3) . y_test.shape . (12630,) . IMAGE EXPLORATION . i = 1000 plt.imshow(X_train[i]) y_train[i] . 36 . i = 3390 plt.imshow(X_train[i]) y_train[i] . 1 . i = 5000 plt.imshow(X_train[i]) y_train[i] . 37 . DATA PREPARATION . Shuffling the data . from sklearn.utils import shuffle X_train, y_train = shuffle(X_train, y_train) . Convert the colored images to Grayscale image. . (Averaging the pixels) . X_train_gray = np.sum(X_train/3, axis=3, keepdims=True) X_test_gray = np.sum(X_test/3, axis=3, keepdims=True) X_validation_gray = np.sum(X_validation/3, axis=3, keepdims=True) . X_train_gray.shape . (34799, 32, 32, 1) . X_test_gray.shape . (12630, 32, 32, 1) . X_validation_gray.shape . (4410, 32, 32, 1) . Normalization of the image. . Pixels now ranges in [-1,1] . X_train_gray_norm = (X_train_gray - 128)/128 X_test_gray_norm = (X_test_gray - 128)/128 X_validation_gray_norm = (X_validation_gray - 128)/128 . To double check that we didnt mess up the images . Squeeze to that we do not need the last dimentionality as it is now grayscale and cmap=gray . a) Original Grayscale image. . i = 1000 plt.imshow(X_train_gray[i].squeeze(), cmap=&#39;gray&#39; ) . &lt;matplotlib.image.AxesImage at 0x211a2144b08&gt; . Original Colored Image . plt.imshow(X_train[i]) . &lt;matplotlib.image.AxesImage at 0x211a2297288&gt; . Normalised Grayscale Image . plt.imshow(X_train_gray_norm[i].squeeze(), cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x211a2368108&gt; . MODEL TRAINING . from keras.models import Sequential from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout from keras.optimizers import Adam from keras.callbacks import TensorBoard . cnn_model = Sequential() #SEQUENCIAL LAYER 1 cnn_model.add(Conv2D(filters=6, kernel_size=(5,5), activation=&#39;relu&#39;, input_shape=(32,32,1))) # Downsizing or DownScaling cnn_model.add(AveragePooling2D()) #SEQUENCIAL LAYER 2 cnn_model.add(Conv2D(filters=16 , kernel_size=(5,5), activation=&#39;relu&#39;)) cnn_model.add(AveragePooling2D()) # Flattening the layers cnn_model.add(Flatten()) #Adding the Dense layers cnn_model.add(Dense(units=120, activation=&#39;relu&#39;)) cnn_model.add(Dense(units=84, activation=&#39;relu&#39;)) #Since the output is Categorical, therefore we use softmax/sigmoid activation function. cnn_model.add(Dense(units=43, activation=&#39;softmax&#39;)) . cnn_model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=Adam(lr=0.001),metrics=[&#39;accuracy&#39;]) . Training Model on Dataset . history = cnn_model.fit(X_train_gray_norm, y_train, batch_size = 500, epochs=50, verbose=1, validation_data=(X_validation_gray_norm, y_validation)) . Epoch 1/50 70/70 [==============================] - 1s 14ms/step - loss: 6.9355e-04 - accuracy: 0.9998 - val_loss: 0.6356 - val_accuracy: 0.9293 Epoch 2/50 70/70 [==============================] - 1s 11ms/step - loss: 1.1938e-04 - accuracy: 1.0000 - val_loss: 0.6383 - val_accuracy: 0.9290 Epoch 3/50 70/70 [==============================] - 1s 11ms/step - loss: 9.2450e-05 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.9295 Epoch 4/50 70/70 [==============================] - 1s 11ms/step - loss: 8.0247e-05 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.9288 Epoch 5/50 70/70 [==============================] - 1s 11ms/step - loss: 7.1426e-05 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.9283 Epoch 6/50 70/70 [==============================] - 1s 12ms/step - loss: 6.4965e-05 - accuracy: 1.0000 - val_loss: 0.6440 - val_accuracy: 0.9293 Epoch 7/50 70/70 [==============================] - 1s 11ms/step - loss: 5.9861e-05 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.9288 Epoch 8/50 70/70 [==============================] - 1s 11ms/step - loss: 5.5509e-05 - accuracy: 1.0000 - val_loss: 0.6460 - val_accuracy: 0.9288 Epoch 9/50 70/70 [==============================] - 1s 11ms/step - loss: 5.1831e-05 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.9286 Epoch 10/50 70/70 [==============================] - 1s 11ms/step - loss: 4.8611e-05 - accuracy: 1.0000 - val_loss: 0.6472 - val_accuracy: 0.9286 Epoch 11/50 70/70 [==============================] - 1s 11ms/step - loss: 4.5816e-05 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 0.9283 Epoch 12/50 70/70 [==============================] - 1s 11ms/step - loss: 4.3274e-05 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.9283 Epoch 13/50 70/70 [==============================] - 1s 11ms/step - loss: 4.1076e-05 - accuracy: 1.0000 - val_loss: 0.6496 - val_accuracy: 0.9281 Epoch 14/50 70/70 [==============================] - 1s 11ms/step - loss: 3.8920e-05 - accuracy: 1.0000 - val_loss: 0.6503 - val_accuracy: 0.9281 Epoch 15/50 70/70 [==============================] - 1s 11ms/step - loss: 3.7074e-05 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 0.9283 Epoch 16/50 70/70 [==============================] - 1s 11ms/step - loss: 3.5310e-05 - accuracy: 1.0000 - val_loss: 0.6513 - val_accuracy: 0.9283 Epoch 17/50 70/70 [==============================] - 1s 11ms/step - loss: 3.3657e-05 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.9281 Epoch 18/50 70/70 [==============================] - 1s 11ms/step - loss: 3.2189e-05 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 0.9281 Epoch 19/50 70/70 [==============================] - 1s 11ms/step - loss: 3.0746e-05 - accuracy: 1.0000 - val_loss: 0.6539 - val_accuracy: 0.9286 Epoch 20/50 70/70 [==============================] - 1s 11ms/step - loss: 2.9457e-05 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.9290 Epoch 21/50 70/70 [==============================] - 1s 12ms/step - loss: 2.8221e-05 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.9290 Epoch 22/50 70/70 [==============================] - 1s 11ms/step - loss: 2.7057e-05 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.9290 Epoch 23/50 70/70 [==============================] - 1s 11ms/step - loss: 2.5983e-05 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.9290 Epoch 24/50 70/70 [==============================] - 1s 11ms/step - loss: 2.4963e-05 - accuracy: 1.0000 - val_loss: 0.6579 - val_accuracy: 0.9288 Epoch 25/50 70/70 [==============================] - 1s 11ms/step - loss: 2.3994e-05 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.9295 Epoch 26/50 70/70 [==============================] - 1s 12ms/step - loss: 2.3091e-05 - accuracy: 1.0000 - val_loss: 0.6594 - val_accuracy: 0.9295 Epoch 27/50 70/70 [==============================] - 1s 14ms/step - loss: 2.2212e-05 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.9295 Epoch 28/50 70/70 [==============================] - 1s 11ms/step - loss: 2.1396e-05 - accuracy: 1.0000 - val_loss: 0.6609 - val_accuracy: 0.9297 Epoch 29/50 70/70 [==============================] - 1s 12ms/step - loss: 2.0623e-05 - accuracy: 1.0000 - val_loss: 0.6616 - val_accuracy: 0.9295 Epoch 30/50 70/70 [==============================] - 1s 12ms/step - loss: 1.9836e-05 - accuracy: 1.0000 - val_loss: 0.6621 - val_accuracy: 0.9299 Epoch 31/50 70/70 [==============================] - 1s 12ms/step - loss: 1.9176e-05 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 0.9299 Epoch 32/50 70/70 [==============================] - 1s 12ms/step - loss: 1.8455e-05 - accuracy: 1.0000 - val_loss: 0.6634 - val_accuracy: 0.9299 Epoch 33/50 70/70 [==============================] - 1s 12ms/step - loss: 1.7789e-05 - accuracy: 1.0000 - val_loss: 0.6641 - val_accuracy: 0.9299 Epoch 34/50 70/70 [==============================] - 1s 14ms/step - loss: 1.7174e-05 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 0.9299 Epoch 35/50 70/70 [==============================] - 1s 11ms/step - loss: 1.6582e-05 - accuracy: 1.0000 - val_loss: 0.6653 - val_accuracy: 0.9299 Epoch 36/50 70/70 [==============================] - 1s 11ms/step - loss: 1.6029e-05 - accuracy: 1.0000 - val_loss: 0.6659 - val_accuracy: 0.9299 Epoch 37/50 70/70 [==============================] - 1s 11ms/step - loss: 1.5489e-05 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 0.9299 Epoch 38/50 70/70 [==============================] - 1s 11ms/step - loss: 1.4962e-05 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 0.9299 Epoch 39/50 70/70 [==============================] - 1s 10ms/step - loss: 1.4464e-05 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 0.9302 Epoch 40/50 70/70 [==============================] - 1s 11ms/step - loss: 1.3998e-05 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.9299 Epoch 41/50 70/70 [==============================] - 1s 11ms/step - loss: 1.3548e-05 - accuracy: 1.0000 - val_loss: 0.6692 - val_accuracy: 0.9299 Epoch 42/50 70/70 [==============================] - 1s 11ms/step - loss: 1.3082e-05 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 0.9297 Epoch 43/50 70/70 [==============================] - 1s 11ms/step - loss: 1.2651e-05 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 0.9299 Epoch 44/50 70/70 [==============================] - 1s 11ms/step - loss: 1.2249e-05 - accuracy: 1.0000 - val_loss: 0.6715 - val_accuracy: 0.9302 Epoch 45/50 70/70 [==============================] - 1s 11ms/step - loss: 1.1872e-05 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 0.9302 Epoch 46/50 70/70 [==============================] - 1s 11ms/step - loss: 1.1498e-05 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.9302 Epoch 47/50 70/70 [==============================] - 1s 10ms/step - loss: 1.1149e-05 - accuracy: 1.0000 - val_loss: 0.6739 - val_accuracy: 0.9299 Epoch 48/50 70/70 [==============================] - 1s 11ms/step - loss: 1.0780e-05 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.9299 Epoch 49/50 70/70 [==============================] - 1s 11ms/step - loss: 1.0444e-05 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.9304 Epoch 50/50 70/70 [==============================] - 1s 10ms/step - loss: 1.0119e-05 - accuracy: 1.0000 - val_loss: 0.6765 - val_accuracy: 0.9306 . MODEL EVALUATION . score = cnn_model.evaluate(X_test_gray_norm, y_test) print(&#39;Test Accuracy: {}&#39;.format(score[1])) . 395/395 [==============================] - 3s 7ms/step - loss: 1.0971 - accuracy: 0.9229 Test Accuracy: 0.9228820204734802 . history.history.keys() . dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;]) . accuracy = history.history[&#39;accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] val_accuracy = history.history[&#39;val_accuracy&#39;] . epochs = range(len(accuracy)) plt.plot(epochs, accuracy, &#39;bo&#39;, label = &#39;Training Accuracy&#39;) plt.plot(epochs, val_accuracy, &#39;b&#39;, label = &#39;Training Validation Accuracy&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x211c5ef3a48&gt; . plt.plot(epochs, loss, &#39;ro&#39;, label = &#39;Training Loss&#39;) plt.plot(epochs, val_loss, &#39;r&#39;, label = &#39;Training validation loss&#39;) plt.title(&#39;Training and validation loss&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x211c60731c8&gt; . Evaluation using the Confusion Metrix . predicted_classes = cnn_model.predict_classes(X_test_gray_norm) y_true = y_test . c: users shekhu anaconda3 envs neural lib site-packages tensorflow python keras engine sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`, if your model does multi-class classification (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) &gt; 0.5).astype(&#34;int32&#34;)`, if your model does binary classification (e.g. if it uses a `sigmoid` last-layer activation). warnings.warn(&#39;`model.predict_classes()` is deprecated and &#39; . from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_true, predicted_classes) plt.figure(figsize=(25,25)) sns.heatmap(cm, annot=True) . &lt;AxesSubplot:&gt; . L =7 W = 7 fig, axes = plt.subplots(L, W, figsize=(12,12)) axes = axes.ravel() for i in np.arange(0, L*W): axes[i].imshow(X_test[i]) axes[i].set_title(&#39;Prediction = {} n True = {}&#39;.format(predicted_classes[i], y_true[i])) axes[i].axis(&#39;off&#39;) .",
            "url": "https://siddiquisuhail.github.io/traffic_signal_classification/2021/06/01/Traffic=Signal-Classification-Using-La-Net-Architecture.html",
            "relUrl": "/2021/06/01/Traffic=Signal-Classification-Using-La-Net-Architecture.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://siddiquisuhail.github.io/traffic_signal_classification/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://siddiquisuhail.github.io/traffic_signal_classification/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://siddiquisuhail.github.io/traffic_signal_classification/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://siddiquisuhail.github.io/traffic_signal_classification/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}